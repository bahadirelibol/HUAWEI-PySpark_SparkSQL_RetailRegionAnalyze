{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 6px solid #C2172D;\">\n",
    "    <h2 style=\"color: black\" id=\"introduction\">Batch Data Processing with Apache Spark</h2>\n",
    "    <p></p>\n",
    "</div>\n",
    "\n",
    "-------------------------\n",
    "**YASİN BAHADIR ELİBOL**\n",
    "-------------------------\n",
    "bahadirelibol60@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Contents](#contents)\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importing the libraries](#library)\n",
    "3. [Reading the data](#read_data)\n",
    "4. [SparkSQL Practices](#spark_sql_practices)\n",
    "   * [Selecting columns](#selecting_columns)\n",
    "   * [Data manipulation](#data_manipulation)\n",
    "   * [Filtering rows](#filtering_rows)\n",
    "   * [Aggregating data](#aggregating_data)\n",
    "   * [Joining](#joining)\n",
    "5. [Case Studies](#assignments)\n",
    "   * [Assignment 1: Jacket sales per region](#assignment_1)\n",
    "   * [Assignment 2: Maximum turnover of the retailer regions](#assignment_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"introduction\">\n",
    "        <h3 style=\"color: #C2172D\">1. Introduction</h3>\n",
    "    </a>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/data_model.svg\"  style=\"width:1000px; padding: 20px\"/>\n",
    "\n",
    "#### SQL Tables Description\n",
    "- **FactSale:** Sales transactions fact table\n",
    "- **FactPurchase:** Purchases fact table\n",
    "- **DimRetailer:** Retailer details dimension table\n",
    "- **DimCustomer:** Customer details dimension table\n",
    "- **DimProduct:** Product details dimension table\n",
    "- **DimRegion:** Region details dimension table\n",
    "- **DimDate:** Date dimension table\n",
    "- **DimSupplier:** Supplier details dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"library\">\n",
    "        <h3 style=\"color: #C2172D\">2. Importing the libraries</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"read_data\">\n",
    "        <h3 style=\"color: #C2172D\">3. Reading the data</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yeni SparkSession örneği oluşturma\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "#Parke verilerini okuma ve DataFrame değişkenlerine atama\n",
    "df_pur = spark.read.parquet(\"data/purchase\")\n",
    "df_sal = spark.read.parquet(\"data/sale\")\n",
    "df_cus = spark.read.parquet(\"data/customer\")\n",
    "df_ret = spark.read.parquet(\"C:/Users/BAHADIR/Desktop/huawei/odev/data/retailer/retail.parquet\") #benim dosya yolum\n",
    "df_pro = spark.read.parquet(\"data/product\")\n",
    "df_sup = spark.read.parquet(\"data/supplier\")\n",
    "df_reg = spark.read.parquet(\"data/region\")\n",
    "df_date = spark.read.parquet(\"data/date\")\n",
    "\n",
    "#Spark SQL sorguları için geçici görünüm tabloları oluşturma\n",
    "df_cus.createOrReplaceTempView(\"DimCustomer\")\n",
    "df_pur.createOrReplaceTempView(\"FactPurchase\")\n",
    "df_sal.createOrReplaceTempView(\"FactSale\")\n",
    "df_ret.createOrReplaceTempView(\"DimRetailer\")\n",
    "df_pro.createOrReplaceTempView(\"DimProduct\")\n",
    "df_sup.createOrReplaceTempView(\"DimSupplier\")\n",
    "df_reg.createOrReplaceTempView(\"DimRegion\")\n",
    "df_date.createOrReplaceTempView(\"DimDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"spark_sql_practices\">\n",
    "        <h3 style=\"color: #C2172D\">4. Spark SQL Practices</h3>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"selecting_columns\">Selecting columns</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+----------+\n",
      "|customer_id|   name| surname|birth_date|\n",
      "+-----------+-------+--------+----------+\n",
      "|          1| Jazmin|  Burril|1958-09-22|\n",
      "|          2| Dalila|   Faers|2000-11-08|\n",
      "|          3|Wayland|Walework|1976-03-08|\n",
      "|          4|Amberly|  Haquin|1948-10-08|\n",
      "|          5|Garrett|   Frear|1957-09-25|\n",
      "+-----------+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT customer_id, name, surname, birth_date FROM DimCustomer LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+----------+\n",
      "|customer_id|   name| surname|birth_date|\n",
      "+-----------+-------+--------+----------+\n",
      "|          1| Jazmin|  Burril|1958-09-22|\n",
      "|          2| Dalila|   Faers|2000-11-08|\n",
      "|          3|Wayland|Walework|1976-03-08|\n",
      "|          4|Amberly|  Haquin|1948-10-08|\n",
      "|          5|Garrett|   Frear|1957-09-25|\n",
      "+-----------+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cus.select(\"customer_id\", \"name\", \"surname\", \"birth_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"data_manipulation\">Data manipulation: </a>** Calculating the ages from date of birth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+---+\n",
      "|customer_id|   name| surname|age|\n",
      "+-----------+-------+--------+---+\n",
      "|          1| Jazmin|  Burril| 67|\n",
      "|          2| Dalila|   Faers| 25|\n",
      "|          3|Wayland|Walework| 49|\n",
      "|          4|Amberly|  Haquin| 77|\n",
      "|          5|Garrett|   Frear| 68|\n",
      "+-----------+-------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    customer_id\n",
    "    ,name\n",
    "    ,surname\n",
    "    ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
    "FROM DimCustomer\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+---+\n",
      "|customer_id|   name| surname|age|\n",
      "+-----------+-------+--------+---+\n",
      "|          1| Jazmin|  Burril| 67|\n",
      "|          2| Dalila|   Faers| 25|\n",
      "|          3|Wayland|Walework| 49|\n",
      "|          4|Amberly|  Haquin| 77|\n",
      "|          5|Garrett|   Frear| 68|\n",
      "+-----------+-------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
    "    .select(\"customer_id\", \"name\", \"surname\", \"age\")\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"filtering_rows\">Filtering rows</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+\n",
      "|   name| surname|age|\n",
      "+-------+--------+---+\n",
      "| Jazmin|  Burril| 67|\n",
      "|Wayland|Walework| 49|\n",
      "|Amberly|  Haquin| 77|\n",
      "|Garrett|   Frear| 68|\n",
      "|  Horst|   Isted| 50|\n",
      "+-------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    name\n",
    "    ,surname\n",
    "    ,age\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "        customer_id\n",
    "        ,name\n",
    "        ,surname\n",
    "        ,YEAR(CURRENT_DATE()) - YEAR(birth_date) AS age\n",
    "    FROM DimCustomer\n",
    ")\n",
    "WHERE age >= 30\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---+\n",
      "|   name| surname|age|\n",
      "+-------+--------+---+\n",
      "| Jazmin|  Burril| 67|\n",
      "|Wayland|Walework| 49|\n",
      "|Amberly|  Haquin| 77|\n",
      "|Garrett|   Frear| 68|\n",
      "|  Horst|   Isted| 50|\n",
      "+-------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus.withColumn(\"age\", F.year(F.current_date()) - F.year(\"birth_date\"))\n",
    "    .select(\"name\", \"surname\", \"age\")\n",
    "    .filter(F.col(\"age\") >= 30)\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"aggregating_data\">Aggregating data</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "fact\n",
    "inner join\n",
    "dim\n",
    "--\n",
    "fact\n",
    "left join \n",
    "dim\n",
    "--\n",
    "fact \n",
    "right joinm\n",
    "dim\n",
    "--\n",
    "fact\n",
    "left outer join\n",
    "\n",
    "cross join \n",
    " window function ?\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|order_id|total_quantity|total_amount|\n",
      "+--------+--------------+------------+\n",
      "|    3647|            13|         521|\n",
      "|    2574|            13|         488|\n",
      "|    3515|            13|         402|\n",
      "|     101|            12|         359|\n",
      "|     440|            12|         426|\n",
      "|    3763|            12|         323|\n",
      "|    1585|            12|         488|\n",
      "|    3289|            12|         327|\n",
      "|    2337|            11|         357|\n",
      "|    3743|            11|         359|\n",
      "+--------+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_sal.groupBy(\"order_id\").agg(\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.sum(\"total_amt\").alias(\"total_amount\")\n",
    "    ).orderBy(\"total_quantity\", ascending=False)\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<a id=\"joining\">Joining</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|      region_name|               age|\n",
      "+-----------------+------------------+\n",
      "|          Akdeniz| 51.81521739130435|\n",
      "|     Dogu Anadolu| 51.13095238095238|\n",
      "|Guneydogu Anadolu| 49.58119658119658|\n",
      "|          Marmara|49.189542483660134|\n",
      "|       Ic Anadolu| 49.07772020725388|\n",
      "|        Karadeniz| 48.75121951219512|\n",
      "|              Ege|47.888888888888886|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    region_name\n",
    "    ,AVG(YEAR(CURRENT_DATE()) - YEAR(birth_date)) AS age\n",
    "FROM DimCustomer cus\n",
    "INNER JOIN DimRegion reg\n",
    "ON cus.city_id = reg.city_id\n",
    "GROUP BY region_name\n",
    "ORDER BY age DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|      region_name|               age|\n",
      "+-----------------+------------------+\n",
      "|          Akdeniz| 51.81521739130435|\n",
      "|     Dogu Anadolu| 51.13095238095238|\n",
      "|Guneydogu Anadolu| 49.58119658119658|\n",
      "|          Marmara|49.189542483660134|\n",
      "|       Ic Anadolu| 49.07772020725388|\n",
      "|        Karadeniz| 48.75121951219512|\n",
      "|              Ege|47.888888888888886|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_cus\n",
    "    .join(df_reg, df_cus.city_id == df_reg.city_id)\n",
    "    .groupBy(\"region_name\").agg(\n",
    "        F.avg(F.year(F.current_date()) - F.year(\"birth_date\")).alias(\"age\")\n",
    "    )\n",
    "    .orderBy(\"age\", ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px; border-bottom: 4px solid #C2172D;\">\n",
    "    <a id=\"case_studies\">\n",
    "        <h3 style=\"color: #C2172D\">5. Case Studies</h3>\n",
    "    </a>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px;\">\n",
    "    <a id=\"assignment_1\">\n",
    "        <h4 style=\"color: #0D9276\">Assignment 1: Jacket sales per region</h3>\n",
    "    </a>\n",
    "</div>\n",
    "<br>\n",
    "<h4>\n",
    "    Write SparkSQL scripts that results: Region-based total quantity and amount of jacket sales between June and August 2023.\n",
    "</h4>\n",
    "<p>The expected out is as follows: </p>\n",
    "\n",
    "| region_name       | product_type | total_quantity | total_amount |   |\n",
    "|-------------------|--------------|----------------|--------------|---|\n",
    "| Marmara           | Jacket       | 213            | 8358         |   |\n",
    "| Dogu Anadolu      | Jacket       | 284            | 11547        |   |\n",
    "| Guneydogu Anadolu | Jacket       | 176            | 6981         |   |\n",
    "| Ic Anadolu        | Jacket       | 260            | 10496        |   |\n",
    "| Akdeniz           | Jacket       | 162            | 6637         |   |\n",
    "| Karadeniz         | Jacket       | 310            | 12582        |   |\n",
    "| Ege               | Jacket       | 101            | 3953      \n",
    "\n",
    "\n",
    "### External links\n",
    "- https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html   |   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+--------------+------------+\n",
      "|      region_name|product_type|total_quantity|total_amount|\n",
      "+-----------------+------------+--------------+------------+\n",
      "|          Marmara|      Jacket|           213|        8358|\n",
      "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
      "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
      "|       Ic Anadolu|      Jacket|           260|       10496|\n",
      "|          Akdeniz|      Jacket|           162|        6637|\n",
      "|        Karadeniz|      Jacket|           310|       12582|\n",
      "|              Ege|      Jacket|           101|        3953|\n",
      "+-----------------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your Spark SQL Solution:\n",
    "#2023 yazında (Haziran-Ağustos), her bölge için kaç ceket satıldığını ve ne kadar para kazanıldığını gösteren bir rapor.\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    reg.region_name,\n",
    "    prod.product_type,\n",
    "    SUM(sal.quantity) AS total_quantity,\n",
    "    SUM(sal.total_amt) AS total_amount\n",
    "FROM FactSale sal\n",
    "JOIN DimProduct prod ON sal.product_id = prod.product_id\n",
    "JOIN DimCustomer cus ON sal.customer_id = cus.customer_id\n",
    "JOIN DimRegion reg ON cus.city_id = reg.city_id\n",
    "JOIN DimDate dt ON sal.date = dt.date\n",
    "WHERE prod.product_type = 'Jacket'\n",
    "  AND dt.year = 2023\n",
    "  AND dt.month BETWEEN 6 AND 8\n",
    "GROUP BY reg.region_name, prod.product_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+--------------+------------+\n",
      "|      region_name|product_type|total_quantity|total_amount|\n",
      "+-----------------+------------+--------------+------------+\n",
      "|          Marmara|      Jacket|           213|        8358|\n",
      "|     Dogu Anadolu|      Jacket|           284|       11547|\n",
      "|Guneydogu Anadolu|      Jacket|           176|        6981|\n",
      "|       Ic Anadolu|      Jacket|           260|       10496|\n",
      "|          Akdeniz|      Jacket|           162|        6637|\n",
      "|        Karadeniz|      Jacket|           310|       12582|\n",
      "|              Ege|      Jacket|           101|        3953|\n",
      "+-----------------+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your PySpark Solution:\n",
    "(\n",
    "    df_sal\n",
    "    .join(df_pro, df_sal.product_id == df_pro.product_id)\n",
    "    .join(df_cus, df_sal.customer_id == df_cus.customer_id)\n",
    "    .join(df_reg, df_cus.city_id == df_reg.city_id)\n",
    "    .join(df_date, df_sal.date == df_date.date)\n",
    "    .filter(\n",
    "        (F.col(\"product_type\") == \"Jacket\") &\n",
    "        (F.col(\"year\") == 2023) &\n",
    "        (F.col(\"month\").between(6, 8))\n",
    "    )\n",
    "    .groupBy(\"region_name\", \"product_type\")\n",
    "    .agg(\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.sum(\"total_amt\").alias(\"total_amount\")\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; padding: 10px;\">\n",
    "    <a id=\"assignment_2\">\n",
    "        <h4 style=\"color: #0D9276\">Assignment 2: Maximum turnover of the retailer regions</h3>\n",
    "    </a>\n",
    "</div>\n",
    "<br>\n",
    "<h4>\n",
    "    Find the maximum turnover region of each retailer, and obtain total amount for each retailer and region.\n",
    "</h4>\n",
    "<p>The expected out is as follows: </p>\n",
    "\n",
    "| retailer_id | retailer_name | region_name | total_amount |\n",
    "|-------------|---------------|-------------|--------------|\n",
    "| 1           | A             | Karadeniz   | 42642        |\n",
    "| 2           | B             | Ic Anadolu  | 71689        |\n",
    "| 3           | C             | Ic Anadolu  | 11995        |\n",
    "| 4           | C             | Karadeniz   | 16081        |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------------+\n",
      "|retailer_id|retailer_name|region_name|total_amount|\n",
      "+-----------+-------------+-----------+------------+\n",
      "|          1|            A|  Karadeniz|       42642|\n",
      "|          2|            B| Ic Anadolu|       71689|\n",
      "|          3|            C| Ic Anadolu|       11995|\n",
      "|          4|            D|  Karadeniz|       16081|\n",
      "+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your Spark SQL Solution:\n",
    "#Her perakendecinin en çok satış yaptığı bölgeyi bulma ve gösterme.\n",
    "spark.sql(\"\"\"\n",
    "WITH regional_sales AS (\n",
    "    SELECT \n",
    "        ret.retailer_id,\n",
    "        ret.retailer_name,\n",
    "        reg.region_name,\n",
    "        SUM(sal.total_amt) AS total_amount\n",
    "    FROM FactSale sal\n",
    "    JOIN DimCustomer cus ON sal.customer_id = cus.customer_id\n",
    "    JOIN DimRegion reg ON cus.city_id = reg.city_id\n",
    "    JOIN DimRetailer ret ON sal.retailer_id = ret.retailer_id\n",
    "    GROUP BY ret.retailer_id, ret.retailer_name, reg.region_name\n",
    "),\n",
    "ranked_sales AS (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER(PARTITION BY retailer_id ORDER BY total_amount DESC) AS rn\n",
    "    FROM regional_sales\n",
    ")\n",
    "SELECT retailer_id, retailer_name, region_name, total_amount\n",
    "FROM ranked_sales\n",
    "WHERE rn = 1\n",
    "ORDER BY retailer_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+------------+\n",
      "|retailer_id|retailer_name|region_name|total_amount|\n",
      "+-----------+-------------+-----------+------------+\n",
      "|          1|            A|  Karadeniz|       42642|\n",
      "|          2|            B| Ic Anadolu|       71689|\n",
      "|          3|            C| Ic Anadolu|       11995|\n",
      "|          4|            D|  Karadeniz|       16081|\n",
      "+-----------+-------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your PySpark Solution:\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "regional_sales_df = (\n",
    "    df_sal\n",
    "    .join(df_cus, df_sal.customer_id == df_cus.customer_id)\n",
    "    .join(df_reg, df_cus.city_id == df_reg.city_id)\n",
    "    .join(df_ret, df_sal.retailer_id == df_ret.retailer_id)\n",
    "    .groupBy(df_ret.retailer_id, \"retailer_name\", \"region_name\")\n",
    "    .agg(F.sum(\"total_amt\").alias(\"total_amount\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"retailer_id\").orderBy(F.desc(\"total_amount\"))\n",
    "\n",
    "top_regions_df = (\n",
    "    regional_sales_df\n",
    "    .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .select(\"retailer_id\", \"retailer_name\", \"region_name\", \"total_amount\")\n",
    "    .orderBy(\"retailer_id\")\n",
    ")\n",
    "top_regions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
